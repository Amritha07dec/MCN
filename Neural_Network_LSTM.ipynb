{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNDw9Z6Zt8y7uf7W8fTTCRM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Amritha07dec/MCN/blob/main/Neural_Network_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d68lO8J6eA3b"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def pad_samples(samples):\n",
        "    \"\"\"\n",
        "    Pads all samples to the same number of features (columns) using zeros.\n",
        "\n",
        "    Parameters:\n",
        "        samples (List of np.arrays): Each array is (time_steps, num_features)\n",
        "\n",
        "    Returns:\n",
        "        padded_samples (np.array): Shape = (num_samples, time_steps, max_features)\n",
        "    \"\"\"\n",
        "    max_features = max(sample.shape[1] for sample in samples)\n",
        "    padded_samples = []\n",
        "\n",
        "    for sample in samples:\n",
        "        time_steps, feat = sample.shape\n",
        "        pad_width = max_features - feat\n",
        "        if pad_width > 0:\n",
        "            padding = np.zeros((time_steps, pad_width))\n",
        "            padded = np.concatenate([sample, padding], axis=1)\n",
        "        else:\n",
        "            padded = sample  # already max_features\n",
        "        padded_samples.append(padded)\n",
        "\n",
        "    return np.array(padded_samples)  # shape = (num_samples, time_steps, max_features)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class TimeSeriesDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        \"\"\"\n",
        "        X: numpy array of shape (samples, time_steps, features)\n",
        "        y: numpy array of shape (samples,) with class labels 0-3\n",
        "        \"\"\"\n",
        "        self.X = torch.tensor(X, dtype=torch.float32)\n",
        "        self.y = torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n"
      ],
      "metadata": {
        "id": "Slid3b_y4u4-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class LSTMClassifier(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size=64, num_layers=2, num_classes=4):\n",
        "        super(LSTMClassifier, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, (hn, cn) = self.lstm(x)\n",
        "        out = self.dropout(hn[-1])  # final hidden state\n",
        "        out = self.fc(out)\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "A6wxUNAo4xLV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def train(model, dataloader, criterion, optimizer, epochs=10):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for X_batch, y_batch in dataloader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(X_batch)\n",
        "            loss = criterion(outputs, y_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        print(f\"Epoch {epoch+1}, Loss: {total_loss / len(dataloader):.4f}\")\n"
      ],
      "metadata": {
        "id": "LtfG3HAg43Pn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Padding function\n",
        "def pad_samples(raw_samples, target_dim=6):\n",
        "    padded = []\n",
        "    for ts in raw_samples:\n",
        "        pad_width = target_dim - ts.shape[1]\n",
        "        if pad_width > 0:\n",
        "            ts = np.pad(ts, ((0, 0), (0, pad_width)), mode='constant')\n",
        "        padded.append(ts)\n",
        "    return np.array(padded)\n",
        "\n",
        "# Step 2: Dataset class\n",
        "class TimeSeriesDataset(Dataset):\n",
        "    def __init__(self, samples, labels):\n",
        "        self.samples = torch.tensor(samples, dtype=torch.float32)\n",
        "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.samples[idx], self.labels[idx]\n",
        "\n",
        "# Step 3: LSTM model\n",
        "class LSTMClassifier(nn.Module):\n",
        "    def __init__(self, input_size=6, hidden_size=64, num_layers=1, num_classes=4):\n",
        "        super(LSTMClassifier, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        _, (h_n, _) = self.lstm(x)\n",
        "        out = self.fc(h_n[-1])\n",
        "        return out\n",
        "\n",
        "# Step 4: Training loop\n",
        "def train(model, dataloader, criterion, optimizer, epochs=10):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for inputs, targets in dataloader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct += (predicted == targets).sum().item()\n",
        "            total += targets.size(0)\n",
        "\n",
        "        acc = 100 * correct / total\n",
        "        print(f\"Epoch {epoch+1}/{epochs} | Loss: {total_loss:.4f} | Accuracy: {acc:.2f}%\")\n"
      ],
      "metadata": {
        "id": "fXww3Ijg-7bH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}